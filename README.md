# AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models
<div align="center">
Yifei Gao<sup>1</sup>, Jiaqi Wang<sup>1</sup>, Zhiyu Lin<sup>1</sup> and, Jitao Sang*<sup>1,2</sup>
</div>
<div align="center">
<sup>1</sup>Beijing Jiaotong University    <sup>2</sup>Peng cheng Laboratory
</div>
<div align="center">
*Corresponding Author
</div>

<!-- Arxiv Link, Project Link -->
<div style='display:flex; gap: 0.25rem; '>
<a href="https://arxiv.org/abs/2403.08542"><img src="https://img.shields.io/badge/arXiv-2403.08542-b31b1b.svg"></a>
<a href="https://github.com/LucusFigoGao/AIGCs_Confuse_AI_Too"><img src="https://img.shields.io/badge/Project%20Page-online-brightgreen"></a>
<a href='LICENSE'><img src='https://img.shields.io/badge/License-MIT-blue.svg'></a>
</div>

## Introduction
Despite the prosperity of generative models, the risks and challenges posed by AIGC cannot be overlooked. This paper pioneers an exploration into the impact of synthetic images on hallucination problems during the reasoning process of Large Vision Language Models. Extensive experimental results have confirmed a significant deviation between synthetic image- and natural image-induced hallucination, referring to as the hallucination bias.

## Update
- [9/1] Source code of ImageTranslation will be released soon.
- [9/1] We release the synthetic image dataset FakeAMBER [LINK](https://pan.baidu.com/s/1WfqVhXqeQudMFMMrWA4r1w?pwd=d54y).
- [7/16] üéâüéâüéâ Our work is **accepted** to ACMMM 2024ÔºÅ
- [3/8] AIGCs_Confuse_AI_too Paper online [LINK](https://arxiv.org/abs/2403.08542).

## Image Download
* Download the images from this [LINK](https://pan.baidu.com/s/1WfqVhXqeQudMFMMrWA4r1w?pwd=d54y).
* The annotation information is same as [AMBER](https://github.com/junyangwang0410/AMBER/tree/master).

## Overview

## Results
